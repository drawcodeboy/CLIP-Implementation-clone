{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594b5628-ca5d-4c0e-923c-b69133c673b6",
   "metadata": {},
   "source": [
    "# Clone config.py script\n",
    "* 클론하는 리포지토리의 config.py script\n",
    "* 즉, CFG로 작성된 부분은 아래 값들을 확인하여 수정할 것\n",
    "* 작성자가 주피터가 아니라 파이썬 스크립트로 작성해서 그럼\n",
    "```\n",
    "import torch\n",
    "\n",
    "debug = True\n",
    "image_path = \"C:/Moein/AI/Datasets/Flicker-8k/Images\"\n",
    "captions_path = \"C:/Moein/AI/Datasets/Flicker-8k\"\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-3\n",
    "patience = 2\n",
    "factor = 0.5\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'resnet50'\n",
    "image_embedding = 2048\n",
    "text_encoder_model = \"distilbert-base-uncased\"\n",
    "text_embedding = 768\n",
    "text_tokenizer = \"distilbert-base-uncased\"\n",
    "max_length = 200\n",
    "\n",
    "pretrained = False # for both image encoder and text encoder\n",
    "trainable = False # for both image encoder and text encoder\n",
    "temperature = 1.0\n",
    "\n",
    "# image size\n",
    "size = 224\n",
    "\n",
    "# for projection head; used for both image and text encoders\n",
    "num_projection_layers = 1\n",
    "projection_dim = 256 \n",
    "dropout = 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285744c-9a28-4942-aa90-61406579fbc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T10:25:02.553331Z",
     "iopub.status.busy": "2024-02-11T10:25:02.552337Z",
     "iopub.status.idle": "2024-02-11T10:25:02.565295Z",
     "shell.execute_reply": "2024-02-11T10:25:02.563306Z",
     "shell.execute_reply.started": "2024-02-11T10:25:02.553331Z"
    }
   },
   "source": [
    "# Albumentations\n",
    "* Image Augmentation 라이브러리, 초당 처리 속도가 torchvision보다 빠른 것으로 증명됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed319ce-8a04-4d1f-b18d-3cbc7619db5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:18.882366Z",
     "iopub.status.busy": "2024-02-12T14:15:18.881343Z",
     "iopub.status.idle": "2024-02-12T14:15:23.173863Z",
     "shell.execute_reply": "2024-02-12T14:15:23.172864Z",
     "shell.execute_reply.started": "2024-02-12T14:15:18.882366Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db92d6a-6302-4404-8823-b2de6964b979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:23.176853Z",
     "iopub.status.busy": "2024-02-12T14:15:23.175857Z",
     "iopub.status.idle": "2024-02-12T14:15:25.478698Z",
     "shell.execute_reply": "2024-02-12T14:15:25.477699Z",
     "shell.execute_reply.started": "2024-02-12T14:15:23.176853Z"
    }
   },
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b9795-ea59-4fbf-965f-76bc24cbe968",
   "metadata": {},
   "source": [
    "# To CLIP\n",
    "* image와 text 둘 다 encoding을 해야한다.\n",
    "* Text Encoding = DistilBERT (BERT의 작은 사이즈지만 성능은 그에 준함)\n",
    "* * *\n",
    "### 추가적으로 알게된 사항 (Python's Dictionary <code>.items()</code>)\n",
    "* <code>.items()</code> -> 딕셔너리에 있는 key와 value를 리턴한다.\n",
    "* Reference: https://wikidocs.net/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac828604-0719-4819-b9af-8ff475cd2047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:25.480691Z",
     "iopub.status.busy": "2024-02-12T14:15:25.479694Z",
     "iopub.status.idle": "2024-02-12T14:15:25.492660Z",
     "shell.execute_reply": "2024-02-12T14:15:25.491662Z",
     "shell.execute_reply.started": "2024-02-12T14:15:25.480691Z"
    }
   },
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        # Dataset 선언을 통해 Tokenizer Object를 input으로 받는다.\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=200\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "        image = cv2.imread(f'F:/Doby/CLIP/Flickr8k/Images/{self.image_filenames[idx]}')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        \n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b3437f-d380-4bd3-b185-d7f28759980a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:25.496648Z",
     "iopub.status.busy": "2024-02-12T14:15:25.495652Z",
     "iopub.status.idle": "2024-02-12T14:15:25.505626Z",
     "shell.execute_reply": "2024-02-12T14:15:25.504628Z",
     "shell.execute_reply.started": "2024-02-12T14:15:25.496648Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_transforms(mode='train'):\n",
    "    # Train이나 Test나 전처리 같음\n",
    "    # 따로 Augmentation은 하지 않음\n",
    "    if mode == 'train':\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(224, 224, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(224, 224, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ddb7d0-2038-4424-9be4-27d0b097407b",
   "metadata": {},
   "source": [
    "# Image Encoder\n",
    "* image encoder는 ResNet50을 사용한다.\n",
    "* 클론하는 페이지의 ResNet은 Pre-trained도 없고, 그렇다고 해서 학습하지도 않는다.\n",
    "* 여기서는 ImageNet1K 사전학습한 것을 사용하도록 한다. 그리고, 모델 프리징을 시킨다.\n",
    "* 또한, 페이지에서는 AdaptiveAvgPooling까지 써서 출력 vector의 사이즈가 2048이 되도록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e16b741c-6a65-4804-91f3-09b99c22251a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:25.507620Z",
     "iopub.status.busy": "2024-02-12T14:15:25.507620Z",
     "iopub.status.idle": "2024-02-12T14:15:26.165860Z",
     "shell.execute_reply": "2024-02-12T14:15:26.164862Z",
     "shell.execute_reply.started": "2024-02-12T14:15:25.507620Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet50(weights='IMAGENET1K_V1')\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        self.flatten = nn.Flatten() # Image output Vector size = 2,048\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.flatten(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931a6653-4362-4e85-b36b-fc00d109ba13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:26.168851Z",
     "iopub.status.busy": "2024-02-12T14:15:26.167854Z",
     "iopub.status.idle": "2024-02-12T14:15:28.273223Z",
     "shell.execute_reply": "2024-02-12T14:15:28.270231Z",
     "shell.execute_reply.started": "2024-02-12T14:15:26.168851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ImageEncoder                                  [4, 2048]                 --\n",
       "├─Sequential: 1-1                             [4, 2048, 1, 1]           --\n",
       "│    └─Conv2d: 2-1                            [4, 64, 112, 112]         (9,408)\n",
       "│    └─BatchNorm2d: 2-2                       [4, 64, 112, 112]         (128)\n",
       "│    └─ReLU: 2-3                              [4, 64, 112, 112]         --\n",
       "│    └─MaxPool2d: 2-4                         [4, 64, 56, 56]           --\n",
       "│    └─Sequential: 2-5                        [4, 256, 56, 56]          --\n",
       "│    │    └─Bottleneck: 3-1                   [4, 256, 56, 56]          (75,008)\n",
       "│    │    └─Bottleneck: 3-2                   [4, 256, 56, 56]          (70,400)\n",
       "│    │    └─Bottleneck: 3-3                   [4, 256, 56, 56]          (70,400)\n",
       "│    └─Sequential: 2-6                        [4, 512, 28, 28]          --\n",
       "│    │    └─Bottleneck: 3-4                   [4, 512, 28, 28]          (379,392)\n",
       "│    │    └─Bottleneck: 3-5                   [4, 512, 28, 28]          (280,064)\n",
       "│    │    └─Bottleneck: 3-6                   [4, 512, 28, 28]          (280,064)\n",
       "│    │    └─Bottleneck: 3-7                   [4, 512, 28, 28]          (280,064)\n",
       "│    └─Sequential: 2-7                        [4, 1024, 14, 14]         --\n",
       "│    │    └─Bottleneck: 3-8                   [4, 1024, 14, 14]         (1,512,448)\n",
       "│    │    └─Bottleneck: 3-9                   [4, 1024, 14, 14]         (1,117,184)\n",
       "│    │    └─Bottleneck: 3-10                  [4, 1024, 14, 14]         (1,117,184)\n",
       "│    │    └─Bottleneck: 3-11                  [4, 1024, 14, 14]         (1,117,184)\n",
       "│    │    └─Bottleneck: 3-12                  [4, 1024, 14, 14]         (1,117,184)\n",
       "│    │    └─Bottleneck: 3-13                  [4, 1024, 14, 14]         (1,117,184)\n",
       "│    └─Sequential: 2-8                        [4, 2048, 7, 7]           --\n",
       "│    │    └─Bottleneck: 3-14                  [4, 2048, 7, 7]           (6,039,552)\n",
       "│    │    └─Bottleneck: 3-15                  [4, 2048, 7, 7]           (4,462,592)\n",
       "│    │    └─Bottleneck: 3-16                  [4, 2048, 7, 7]           (4,462,592)\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [4, 2048, 1, 1]           --\n",
       "├─Flatten: 1-2                                [4, 2048]                 --\n",
       "===============================================================================================\n",
       "Total params: 23,508,032\n",
       "Trainable params: 0\n",
       "Non-trainable params: 23,508,032\n",
       "Total mult-adds (G): 16.35\n",
       "===============================================================================================\n",
       "Input size (MB): 2.41\n",
       "Forward/backward pass size (MB): 711.29\n",
       "Params size (MB): 94.03\n",
       "Estimated Total Size (MB): 807.74\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "# Attuibute Error 'int' has no attribute 'numpy' 때문에 torchinfo 사용함\n",
    "# torchinfo가 상위 버전인가\n",
    "\n",
    "ie = ImageEncoder()\n",
    "summary(ie, (4, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a9ae6-a995-4d5a-a9e3-14ac4b879a44",
   "metadata": {},
   "source": [
    "# Text Encoder\n",
    "* <a href=\"https://huggingface.co/transformers/v3.0.2/model_doc/distilbert.html\"><code>DistilBERT</code></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51f0d5c-911e-46ad-a451-74d1fe9a330a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:28.277213Z",
     "iopub.status.busy": "2024-02-12T14:15:28.276215Z",
     "iopub.status.idle": "2024-02-12T14:15:38.950665Z",
     "shell.execute_reply": "2024-02-12T14:15:38.949693Z",
     "shell.execute_reply.started": "2024-02-12T14:15:28.277213Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50560c63-64bc-4b6c-9bf8-1147374f3060",
   "metadata": {},
   "source": [
    "# Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002113ad-c697-43e4-a0ab-b459691d8835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:38.952660Z",
     "iopub.status.busy": "2024-02-12T14:15:38.951662Z",
     "iopub.status.idle": "2024-02-12T14:15:38.962632Z",
     "shell.execute_reply": "2024-02-12T14:15:38.961655Z",
     "shell.execute_reply.started": "2024-02-12T14:15:38.952660Z"
    }
   },
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, 256)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(256, 256)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e01d505-d5a7-4995-9a39-1ccb607a47bb",
   "metadata": {},
   "source": [
    "# CLIP Model\n",
    "* CrossEntropy를 굳이 구현해서 사용하는 이유는?\n",
    "1. 매우 드물지만, 한 Batch 안에 동일한 이미지가 들어갈 수도 있기 때문이다.\n",
    "2. 음... 직접 구현하면서 좀 배우길 바랬다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80839ec3-d1f2-4bb7-ba95-90fc419ef22c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:38.964627Z",
     "iopub.status.busy": "2024-02-12T14:15:38.964627Z",
     "iopub.status.idle": "2024-02-12T14:15:38.970613Z",
     "shell.execute_reply": "2024-02-12T14:15:38.969613Z",
     "shell.execute_reply.started": "2024-02-12T14:15:38.964627Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc6591a4-91d0-453e-9664-ca715f3e1ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:38.980584Z",
     "iopub.status.busy": "2024-02-12T14:15:38.979587Z",
     "iopub.status.idle": "2024-02-12T14:15:39.000530Z",
     "shell.execute_reply": "2024-02-12T14:15:38.999535Z",
     "shell.execute_reply.started": "2024-02-12T14:15:38.980584Z"
    }
   },
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, temperature, image_embedding, text_embedding):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Get Image, Text features through ResNet50, DistilBERT\n",
    "        image_features = self.image_encoder(batch['image'])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "\n",
    "        # Same DIMENSION EMBEDDING\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2.0 * self.temperature, dim=-1\n",
    "        )\n",
    "\n",
    "        texts_loss = self.cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = self.cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss = (images_loss + texts_loss) / 2.0\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "    def cross_entropy(self, preds, targets, reduction='none'):\n",
    "        log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        loss = (-targets * log_softmax(preds)).sum(1)\n",
    "        if reduction == 'none':\n",
    "            return loss\n",
    "        elif reduction == 'mean':\n",
    "            return loss.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d505ab4-c8e7-483f-80db-b89d11fbdfe7",
   "metadata": {},
   "source": [
    "# Check How Embedding Works!\n",
    "* Similarity 계산해서 어떻게 작동하는지 확인\n",
    "* <code>F.softmax</code>의 결과가 1., 0.이라 해서 1, 0이 아니라 극단적이라 그렇게 보일 뿐, 소수점 아래 숫자 있음\n",
    "* <code>b_out</code>의 결과를 보면 납득이 된다.\n",
    "* <b>즉, <u>CLIP의 역할</u>은 <u>Image Embedding과 Text Embedding의 Similarity를 Identity Matrix에 가깝게 학습</u>하는 것!!</b>\n",
    "* 그러면 여기서 궁금한 게 있다 왜 굳이 Image Embedding, Text Embedding의 Similarity를 계산해서 Identity Matrix를 만드는 거지?\n",
    "* 완전한 Identity Matrix가 요구되는 것이 아니기 때문에 (위에서 1.0, 0.0이 아니었듯이) 그런 듯 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d716c44-0b38-4d0c-879f-aceb89977ec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:39.002527Z",
     "iopub.status.busy": "2024-02-12T14:15:39.001528Z",
     "iopub.status.idle": "2024-02-12T14:15:39.062366Z",
     "shell.execute_reply": "2024-02-12T14:15:39.061369Z",
     "shell.execute_reply.started": "2024-02-12T14:15:39.002527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============a_out===============\n",
      "tensor([[262.2000,  -9.8797,  15.3062,   9.6773],\n",
      "        [ -9.8797, 230.2375,  14.4797, -32.6931],\n",
      "        [ 15.3062,  14.4797, 265.3638,  16.0985],\n",
      "        [  9.6773, -32.6931,  16.0985, 231.0343]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "===============b_out===============\n",
      "tensor([[ 18.9918,  22.6922,  11.8289, -25.1275],\n",
      "        [ 10.1902,   5.1853, -14.3637,  10.6142],\n",
      "        [-29.9872, -18.0869,  21.0406,   8.1824],\n",
      "        [ -5.5496, -11.6104,   0.5162,  -2.8999]])\n",
      "tensor([[2.4117e-02, 9.7586e-01, 1.8686e-05, 1.6655e-21],\n",
      "        [3.9451e-01, 2.6451e-03, 8.5594e-12, 6.0284e-01],\n",
      "        [6.9012e-23, 1.0167e-17, 1.0000e+00, 2.6047e-06],\n",
      "        [2.2422e-03, 5.2297e-06, 9.6603e-01, 3.1725e-02]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "dim = 256\n",
    "a = torch.randn(batch_size, dim)\n",
    "b = torch.randn(batch_size, dim)\n",
    "\n",
    "a_out = a @ a.T\n",
    "b_out = a @ b.T\n",
    "\n",
    "print('===============a_out===============') #Best Case\n",
    "print(a_out)\n",
    "print(F.softmax(a_out, dim=-1))\n",
    "print('===============b_out===============')\n",
    "print(b_out)\n",
    "print(F.softmax(b_out, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2074c2-0efe-4a14-9901-a4e970449cae",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8598cfcb-b871-4533-a3c2-4ef6416f9307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:39.063363Z",
     "iopub.status.busy": "2024-02-12T14:15:39.063363Z",
     "iopub.status.idle": "2024-02-12T14:15:39.069346Z",
     "shell.execute_reply": "2024-02-12T14:15:39.068349Z",
     "shell.execute_reply.started": "2024-02-12T14:15:39.063363Z"
    }
   },
   "outputs": [],
   "source": [
    "captions_path = 'F:\\Doby\\CLIP\\Flickr8k\\captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00729319-7d0f-4451-b844-6d1cb76c1309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:39.071342Z",
     "iopub.status.busy": "2024-02-12T14:15:39.070345Z",
     "iopub.status.idle": "2024-02-12T14:15:39.576991Z",
     "shell.execute_reply": "2024-02-12T14:15:39.574995Z",
     "shell.execute_reply.started": "2024-02-12T14:15:39.071342Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "captions = pd.read_csv(captions_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8253917-5deb-4c15-bf55-74f673e6cc40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:39.579981Z",
     "iopub.status.busy": "2024-02-12T14:15:39.578985Z",
     "iopub.status.idle": "2024-02-12T14:15:39.612893Z",
     "shell.execute_reply": "2024-02-12T14:15:39.611897Z",
     "shell.execute_reply.started": "2024-02-12T14:15:39.579981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>40450</td>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>40451</td>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>40452</td>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>40453</td>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>40454</td>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                      image  \\\n",
       "0          0  1000268201_693b08cb0e.jpg   \n",
       "1          1  1000268201_693b08cb0e.jpg   \n",
       "2          2  1000268201_693b08cb0e.jpg   \n",
       "3          3  1000268201_693b08cb0e.jpg   \n",
       "4          4  1000268201_693b08cb0e.jpg   \n",
       "...      ...                        ...   \n",
       "40450  40450   997722733_0cb5439472.jpg   \n",
       "40451  40451   997722733_0cb5439472.jpg   \n",
       "40452  40452   997722733_0cb5439472.jpg   \n",
       "40453  40453   997722733_0cb5439472.jpg   \n",
       "40454  40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "0      A child in a pink dress is climbing up a set o...  \n",
       "1                  A girl going into a wooden building .  \n",
       "2       A little girl climbing into a wooden playhouse .  \n",
       "3      A little girl climbing the stairs to her playh...  \n",
       "4      A little girl in a pink dress going into a woo...  \n",
       "...                                                  ...  \n",
       "40450           A man in a pink shirt climbs a rock face  \n",
       "40451           A man is rock climbing high in the air .  \n",
       "40452  A person in a red shirt climbing up a rock fac...  \n",
       "40453                    A rock climber in a red shirt .  \n",
       "40454  A rock climber practices on a rock climbing wa...  \n",
       "\n",
       "[40455 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = captions.reset_index()\n",
    "captions.rename(columns={'index': 'id'}, inplace=True)\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25b29bab-7832-43c6-8a4f-7829b4c8aab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:39.615886Z",
     "iopub.status.busy": "2024-02-12T14:15:39.614888Z",
     "iopub.status.idle": "2024-02-12T14:15:39.626857Z",
     "shell.execute_reply": "2024-02-12T14:15:39.624861Z",
     "shell.execute_reply.started": "2024-02-12T14:15:39.615886Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_train_valid_dfs(df):\n",
    "    max_id = df['id'].max() + 1\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_df = df[df['id'].isin(train_ids)]\n",
    "    train_df = train_df.drop(columns=['id'])\n",
    "    valid_df = df[df['id'].isin(valid_ids)]\n",
    "    valid_df = valid_df.drop(columns=['id'])\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd5e6c1f-5aad-4b19-b33a-9cc51ec84fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:39.629836Z",
     "iopub.status.busy": "2024-02-12T14:15:39.628840Z",
     "iopub.status.idle": "2024-02-12T14:15:40.137479Z",
     "shell.execute_reply": "2024-02-12T14:15:40.136508Z",
     "shell.execute_reply.started": "2024-02-12T14:15:39.629836Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, valid_df = make_train_valid_dfs(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92800ba6-130e-4171-a5f3-4c359f38d40f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:40.138475Z",
     "iopub.status.busy": "2024-02-12T14:15:40.138475Z",
     "iopub.status.idle": "2024-02-12T14:15:40.147453Z",
     "shell.execute_reply": "2024-02-12T14:15:40.146491Z",
     "shell.execute_reply.started": "2024-02-12T14:15:40.138475Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_loaders(df, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        df['image'].values,\n",
    "        df['caption'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=8,\n",
    "        num_workers=0,\n",
    "        shuffle=True if mode == 'train' else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab4786af-78f9-42ef-ab9d-783e78b40803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:40.150447Z",
     "iopub.status.busy": "2024-02-12T14:15:40.149447Z",
     "iopub.status.idle": "2024-02-12T14:15:58.271645Z",
     "shell.execute_reply": "2024-02-12T14:15:58.270678Z",
     "shell.execute_reply.started": "2024-02-12T14:15:40.150447Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_loader = build_loaders(train_df, tokenizer, 'train')\n",
    "valid_loader = build_loaders(valid_df, tokenizer, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0c68765-3ade-4edc-aab0-dc069b0040d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:15:58.272642Z",
     "iopub.status.busy": "2024-02-12T14:15:58.272642Z",
     "iopub.status.idle": "2024-02-12T14:16:00.746027Z",
     "shell.execute_reply": "2024-02-12T14:16:00.745031Z",
     "shell.execute_reply.started": "2024-02-12T14:15:58.272642Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = CLIPModel(temperature=1.0, image_embedding=2048, text_embedding=768).to(device)\n",
    "params = [\n",
    "    {'params': model.image_encoder.parameters(),\n",
    "     'params': model.text_encoder.parameters(),\n",
    "     'params': model.image_projection.parameters(), \n",
    "     'params': model.text_projection.parameters()\n",
    "     }\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3504782-5dec-4f6c-a80a-0955df28f996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:17:14.416026Z",
     "iopub.status.busy": "2024-02-12T14:17:14.415060Z",
     "iopub.status.idle": "2024-02-12T14:17:14.425001Z",
     "shell.execute_reply": "2024-02-12T14:17:14.424033Z",
     "shell.execute_reply.started": "2024-02-12T14:17:14.416026Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, epoch):\n",
    "    n_data = 0\n",
    "    running_loss = 0.\n",
    "    for batch_idx, batch in enumerate(data_loader, start=1):\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'caption'}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_data += 8\n",
    "        running_loss += loss\n",
    "        \n",
    "        print(f'\\rTrain Epoch: {epoch} [{n_data}/{len(data_loader.dataset)} ({100 * batch_idx / len(data_loader):.2f}%)]  Loss: {running_loss/batch_idx:.4f}', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "250d873c-0a2f-425f-827b-7e8c3e3e77f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T14:17:15.404950Z",
     "iopub.status.busy": "2024-02-12T14:17:15.403955Z",
     "iopub.status.idle": "2024-02-12T19:34:53.591296Z",
     "shell.execute_reply": "2024-02-12T19:34:53.589298Z",
     "shell.execute_reply.started": "2024-02-12T14:17:15.404950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32368/32364 (100.00%)]  Loss: 1.8137\n",
      "Train Epoch: 2 [32368/32364 (100.00%)]  Loss: 0.7408\n",
      "Train Epoch: 3 [32368/32364 (100.00%)]  Loss: 0.6437\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3+1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24b34106-476e-4734-a369-14c6686a95fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T19:34:53.597277Z",
     "iopub.status.busy": "2024-02-12T19:34:53.596279Z",
     "iopub.status.idle": "2024-02-12T19:34:57.779141Z",
     "shell.execute_reply": "2024-02-12T19:34:57.775220Z",
     "shell.execute_reply.started": "2024-02-12T19:34:53.597277Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, './CLIPModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "046058dc-4100-4e4f-8442-13039466c98b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-13T01:55:02.055841Z",
     "iopub.status.busy": "2024-02-13T01:55:02.054843Z",
     "iopub.status.idle": "2024-02-13T01:55:06.632607Z",
     "shell.execute_reply": "2024-02-13T01:55:06.631601Z",
     "shell.execute_reply.started": "2024-02-13T01:55:02.055841Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './CLIPModelDict.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
